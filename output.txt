Original run 1: dev EM: 68.61873226111636 F1: 77.89724470528145
Original run 2: dev EM: 68.67549668874172 F1: 78.03895984986448
no Exact match run 1: dev EM: 66.9441816461684 F1: 76.43481553600684
tune all embedding run 1: dev EM: 68.40113528855251 F1: 77.82505881327128
>>> Below all use improved tokenization (handle dashes and [] etc.)

------ Below are run on my original machine (in output.log) ------
Improved tokenization run 1: dev EM: 69.25260170293284 F1: 78.44314735464995
Improved tokenization run 2: dev EM: 69.10122989593188 F1: 78.56207404335898
Random vector for unknown words run 1: dev EM: 69.05392620624409 F1: 78.11589025440102
No word vector pre-alignment run 1: dev EM: 68.39167455061495 F1: 77.55068472418104
Gated input for the first LSTM layer run 1: dev EM: 69.18637653736991 F1: 78.31315547097128
Improved tokenization run 3: dev EM: 69.70671712393566 F1: 78.87747327559043
C2Q attention relu_FC, concat run 1: dev EM: 70.70955534531693 F1: 79.68814222485408
Improved tokenization (skip padding in BiLSTM) run 1: dev EM: 69.18637653736991 F1: 78.70554863209655

------ Below are run on gcrgdw136 (in output_gcrgdw136.log) ------
Improved tokenization (mask input in _forward_unpadded RNN) run 4: dev EM: 68.97824030274361 F1: 78.22092455491139

------ Below are run on toronto-g1 GPU2 (in output_g1_gpu2.log) ------
C2Q attention relu_FC, concat_dot_diff run 1:

------ Below are run on toronto-g1 GPU3 (in output_g1_gpu3.log) ------
C2Q attention relu_FC, concat_dot run 1:

------ Below are run on toronto-g2 GPU5 (in output_g2_gpu5.log) ------
C2Q attention relu_FC, fuse run 1:
